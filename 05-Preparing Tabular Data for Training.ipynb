{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d198fee0",
   "metadata": {},
   "source": [
    "# Preparing Tabular Data with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ba329",
   "metadata": {},
   "source": [
    "Tabular data consist of rows and columns. The values of the categorical columns have to encode as one-hot encoding. In this tutorail, I am going to cover how to preparing tabular data. To show this, I'll use Titanic dataset. First of all, let's import libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba632f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries.\n",
    "import functools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd88a1",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b57ad5",
   "metadata": {},
   "source": [
    "The Titanic dataset is open source and tabular dataset. This dataset consist of columns as such age, gender, cabin grade, and whether or not they survived. Google provide this dataset. Let me create variables that contain URLs of train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5386dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating variables for urls of datasets.\n",
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0992cc",
   "metadata": {},
   "source": [
    "I am going to use get_files() method which downloads a file from a URL if it not already in the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022083db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tf-datasets/titanic/train.csv\n",
      "32768/30874 [===============================] - 0s 3us/step\n",
      "40960/30874 [=======================================] - 0s 2us/step\n",
      "Downloading data from https://storage.googleapis.com/tf-datasets/titanic/eval.csv\n",
      "16384/13049 [=====================================] - 0s 0s/step\n",
      "24576/13049 [========================================================] - 0s 0s/step\n"
     ]
    }
   ],
   "source": [
    "# Creating variables for paths of datasets.\n",
    "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
    "test_file_path = tf.keras.utils.get_file(\"eval.csv\",  TEST_DATA_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8877f70",
   "metadata": {},
   "source": [
    "Pandas is the most popular library of Python. You can manipulate dataset with Pandas. To read these datasets, you can use read_csv () method in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad9766ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting train_file_path into pandas dataframe.\n",
    "train_df = pd.read_csv(train_file_path, header='infer')\n",
    "test_df = pd.read_csv(test_file_path, header='infer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f1b38e",
   "metadata": {},
   "source": [
    "Let me take a look the first five rows of train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa36fda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived     sex   age  n_siblings_spouses  parch     fare  class     deck  \\\n",
       "0         0    male  22.0                   1      0   7.2500  Third  unknown   \n",
       "1         1  female  38.0                   1      0  71.2833  First        C   \n",
       "2         1  female  26.0                   0      0   7.9250  Third  unknown   \n",
       "3         1  female  35.0                   1      0  53.1000  First        C   \n",
       "4         0    male  28.0                   0      0   8.4583  Third  unknown   \n",
       "\n",
       "   embark_town alone  \n",
       "0  Southampton     n  \n",
       "1    Cherbourg     n  \n",
       "2  Southampton     y  \n",
       "3  Southampton     n  \n",
       "4   Queenstown     y  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a look titanic dataset.\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d023ee",
   "metadata": {},
   "source": [
    "## Preprocessing the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fc6116",
   "metadata": {},
   "source": [
    "As you can see above dataseti dataset consist of numeric and categorical columns. You will need to mark \"survived\" columns as the target and mark the rest of the columns as features. To do this I am going to use tf.data.experimental.make_csv_dataset() method. This method reads CSV files into a dataset, where each element of the dataset is a (features, labels) tuple that corresponds to a batch of CSV rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "148b57ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the target and featrues variables.\n",
    "LABEL_COLUMN = 'survived'\n",
    "LABELS = [0, 1]\n",
    "# Let's specify file path, batch size, label name, missing value parameters in make_csv_dataset method.\n",
    "train_ds = tf.data.experimental.make_csv_dataset(\n",
    "        train_file_path,\n",
    "        batch_size = 3,\n",
    "        label_name=LABEL_COLUMN,\n",
    "        na_value=\"?\",\n",
    "        num_epochs= 1,\n",
    "        ignore_errors=True)\n",
    "# Let's create test dataset as above.\n",
    "test_ds = tf.data.experimental.make_csv_dataset(\n",
    "        test_file_path,\n",
    "        batch_size=3,\n",
    "        label_name=LABEL_COLUMN,\n",
    "        na_value=\"?\",\n",
    "        num_epochs=1,\n",
    "        ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a82aa",
   "metadata": {},
   "source": [
    "Let's take a look columns of train dataset in the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bed93dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 0 1], shape=(3,), dtype=int32)\n",
      "sex: [b'male' b'male' b'female']\n",
      "age: [28. 28. 39.]\n",
      "n_siblings_spouses: [0 0 1]\n",
      "parch: [0 1 1]\n",
      "fare: [10.5    33.     83.1583]\n",
      "class: [b'Second' b'Second' b'First']\n",
      "deck: [b'unknown' b'unknown' b'E']\n",
      "embark_town: [b'Southampton' b'Southampton' b'Cherbourg']\n",
      "alone: [b'y' b'n' b'n']\n"
     ]
    }
   ],
   "source": [
    "for batch, label in train_ds.take(1):\n",
    "    print(label)\n",
    "    for key, value in batch.items():\n",
    "        print(f\"{key}: {value.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f94fdb",
   "metadata": {},
   "source": [
    "Now that I loaded train and test datasets. Let me arrange columns by feature types. First of all, I am going to designate numerics columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac3a1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting numeric columns\n",
    "feature_columns = []\n",
    "# numeric columns\n",
    "for header in ['age', 'n_siblings_spouses', 'parch', 'fare']:\n",
    "    feature_columns.append(feature_column.numeric_column(header))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46964746",
   "metadata": {},
   "source": [
    "If you want, you can bin age into a bucket. First, let's take a look statistics of age column. To do this, I am going to use Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be00a4b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c14fb724399c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtitanic_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'infer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "titanic_df = pd.read_csv(train_file_path, header='infer')\n",
    "titanic_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cceeaf9",
   "metadata": {},
   "source": [
    "Let me try three bin boundaries for age : 23, 28, and 35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dff2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketizing age columns\n",
    "age = feature_column.numeric_column('age')\n",
    "age_buckets = feature_column.bucketized_column(age, boundaries=[23, 28, 35])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2570602e",
   "metadata": {},
   "source": [
    "To use one-hot encode, I am going to see the distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a151eefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex : ['male' 'female']\n",
      "class : ['Third' 'First' 'Second']\n",
      "deck : ['unknown' 'C' 'G' 'A' 'B' 'D' 'F' 'E']\n",
      "embark_town : ['Southampton' 'Cherbourg' 'Queenstown' 'unknown']\n",
      "alone : ['n' 'y']\n"
     ]
    }
   ],
   "source": [
    "#Deteriming categorical columns\n",
    "h = {}\n",
    "for col in titanic_df:\n",
    "    if col in ['sex', 'class', 'deck', 'embark_town', 'alone']:\n",
    "        print(col, ':', titanic_df[col].unique())\n",
    "        h[col] = titanic_df[col].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0776314a",
   "metadata": {},
   "source": [
    "Let's use categorical_column_with_vocabulary_list since inputs are in string format. Let me keep track of these unique values using h variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "950b7a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting categorical columns and encoding unique categorical values\n",
    "sex_type = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'Type', h.get('sex').tolist())\n",
    "sex_type_one_hot = feature_column.indicator_column(sex_type)\n",
    "\n",
    "class_type = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'Type', h.get('class').tolist())\n",
    "class_type_one_hot = feature_column.indicator_column(class_type)\n",
    "\n",
    "deck_type = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'Type', h.get('deck').tolist())\n",
    "deck_type_one_hot = feature_column.indicator_column(deck_type)\n",
    "\n",
    "embark_town_type = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'Type', h.get('embark_town').tolist())\n",
    "embark_town_type_one_hot = feature_column.indicator_column(embark_town_type)\n",
    "\n",
    "alone_type = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'Type', h.get('alone').tolist())\n",
    "alone_one_hot = feature_column.indicator_column(alone_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca487f9",
   "metadata": {},
   "source": [
    "\"deck\" column has eight unique values so I am going to embed this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89c49ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeding the \"deck\" column and reducing its dimension to 3.\n",
    "deck = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'deck', titanic_df.deck.unique())\n",
    "deck_embedding = feature_column.embedding_column(deck, dimension=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52d06df",
   "metadata": {},
   "source": [
    "Let's reduce the dimensions of class columns using a hashed feature column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f9d8efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing class column\n",
    "class_hashed = feature_column.categorical_column_with_hash_bucket(\n",
    "      'class', hash_bucket_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa506f",
   "metadata": {},
   "source": [
    "There may be interaction between passenger gender and cabin class. Let's encode those intercations using crossed_column() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73d99d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_type_feature = feature_column.crossed_column(['sex', 'class'], hash_bucket_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e97c298",
   "metadata": {},
   "source": [
    "Now that I am going to put together what I've done. Let's create a list to hold all the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72291e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# appending numeric columns\n",
    "for header in ['age', 'n_siblings_spouses', 'parch', 'fare']:\n",
    "    feature_columns.append(feature_column.numeric_column(header))\n",
    "    \n",
    "# appending bucketized columns\n",
    "age = feature_column.numeric_column('age')\n",
    "age_buckets = feature_column.bucketized_column(age, boundaries=[23, 28, 35])\n",
    "feature_columns.append(age_buckets)\n",
    "\n",
    "# appending categorical columns\n",
    "indicator_column_names = ['sex', 'class', 'deck', 'embark_town', 'alone']\n",
    "for col_name in indicator_column_names:\n",
    "    categorical_column = feature_column.categorical_column_with_vocabulary_list(\n",
    "        col_name, titanic_df[col_name].unique())\n",
    "    indicator_column = feature_column.indicator_column(categorical_column)\n",
    "    feature_columns.append(indicator_column)\n",
    "    \n",
    "# appending embedding columns\n",
    "deck = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'deck', titanic_df.deck.unique())\n",
    "deck_embedding = feature_column.embedding_column(deck, dimension=3)\n",
    "feature_columns.append(deck_embedding)\n",
    "\n",
    "# appending crossed columns\n",
    "feature_columns.append(feature_column.indicator_column(cross_type_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad784e2e",
   "metadata": {},
   "source": [
    "Now I am going to create a feature layer. This layer will serve as the first (input) layer in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c2963af",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b15e6ab",
   "metadata": {},
   "source": [
    "Let me split test_df into validation and test datasets. Hyperparameters are fine tuned using validation dataset and model is evaluated using test dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e01fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df, test_df = train_test_split(test_df, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8946e3b",
   "metadata": {},
   "source": [
    "Let me specify target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a753e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_df.pop(\"survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea8354c",
   "metadata": {},
   "source": [
    "To stream the data into the training process with the dataset, I am going to create a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a2b3e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop('survived')\n",
    "    # To transform the DataFrame into a key-value pair. \n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    # To shuffle and batch\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be390dc9",
   "metadata": {},
   "source": [
    "Appliying this function to both validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68acf5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "val_ds = pandas_to_dataset(val_df, shuffle=False, batch_size = batch_size)\n",
    "test_ds = pandas_to_dataset(test_df, shuffle=False, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614dab9a",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018345ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dropout(.1),\n",
    "  layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b380482a",
   "metadata": {},
   "source": [
    "Take a look summary of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d1c4cc",
   "metadata": {},
   "source": [
    "## Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee6f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e51442",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1eb94f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'sex': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=string>, 'age': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, 'n_siblings_spouses': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=int64>, 'parch': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=int64>, 'fare': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=float64>, 'class': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>, 'deck': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'embark_town': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=string>, 'alone': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'sex': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=string>, 'age': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, 'n_siblings_spouses': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=int64>, 'parch': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=int64>, 'fare': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=float64>, 'class': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>, 'deck': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'embark_town': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=string>, 'alone': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>}\n",
      "Consider rewriting this model with the Functional API.\n",
      " 1/20 [>.............................] - ETA: 14s - loss: 2.5995 - accuracy: 0.4062WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'sex': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=string>, 'age': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, 'n_siblings_spouses': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=int64>, 'parch': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=int64>, 'fare': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=float64>, 'class': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>, 'deck': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'embark_town': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=string>, 'alone': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "20/20 [==============================] - 1s 14ms/step - loss: 1.3593 - accuracy: 0.5981 - val_loss: 0.6715 - val_accuracy: 0.7215\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.8288 - accuracy: 0.6603 - val_loss: 0.5704 - val_accuracy: 0.7215\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7358 - accuracy: 0.6635 - val_loss: 0.5123 - val_accuracy: 0.7278\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6771 - accuracy: 0.6746 - val_loss: 0.4924 - val_accuracy: 0.7278\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6224 - accuracy: 0.6874 - val_loss: 0.4654 - val_accuracy: 0.7342\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6266 - accuracy: 0.7065 - val_loss: 0.4883 - val_accuracy: 0.7468\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6045 - accuracy: 0.7033 - val_loss: 0.4760 - val_accuracy: 0.7658\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5545 - accuracy: 0.7464 - val_loss: 0.4623 - val_accuracy: 0.7405\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5191 - accuracy: 0.7512 - val_loss: 0.4428 - val_accuracy: 0.7911\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5596 - accuracy: 0.7608 - val_loss: 0.5741 - val_accuracy: 0.7405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a93a63f280>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca62152",
   "metadata": {},
   "source": [
    "That is all. In this tutorail, I am going to showed how to prepare tabular dataset to analyze and deal with multiple data types. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c0928d",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb120a8",
   "metadata": {},
   "source": [
    "- [KC Tung, 2021, TensorFlow 2 Pocket Reference](https://www.amazon.com/TensorFlow-Pocket-Reference-Building-Deploying/dp/1492089184)\n",
    "- [TensorFlow Tutorial](https://www.tensorflow.org/tutorials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da4929",
   "metadata": {},
   "source": [
    "Don't forget to follow on Tirendaz Academy [YouTube-Tr](https://youtube.com/c/tirendazakademi), [YouTube-Eng](https://www.youtube.com/channel/UCFU9Go20p01kC64w-tmFORw), [Twitter](https://twitter.com/TirendazAcademy), [Medium](https://tirendazacademy.medium.com), [GitHub](https://github.com/TirendazAcademy) and [LinkedIn](https://www.linkedin.com/in/tirendaz-academy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
